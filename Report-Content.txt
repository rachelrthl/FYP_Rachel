A1. PROBLEM DEFINITION & VALUE PROPOSITION
1.1 The Problem & Persona
Persona: Sarah Tan â€“ Demand Generation Representative, AWS (Singapore) 
Sarah is a Demand Generation Representative (DGR) â€” a front-line outbound sales role responsible for identifying potential customers and booking qualified meetings for Account Executives (AEs). Her performance is measured by the number of â€œsales-qualified meetingsâ€ (SQMs) she creates each week, which directly feeds AWSâ€™s new business pipeline.
Sarah manages a portfolio of 1,200 Small and Medium Business (SMB) accounts, each representing potential cloud deals between US$10Kâ€“100K annually. Her target is to book 10 qualified meetings per week, converting research and outreach into tangible sales opportunities.Within AWSâ€™s structure, DGRs operate under tight time constraints. Interviews with eight AWS sales representatives show that only 12 out of 40 weekly working hours are typically allocated to prospecting. The remaining time is consumed by internal meetings, pipeline reviews, CRM updates, reporting, and follow-ups â€” leaving limited bandwidth for deep account research.
Psychographic Profile:
Sarah is ambitious, data-driven, and goal-oriented. She enjoys helping companies scale through cloud adoption but is often frustrated by the inefficiency of manual prospecting. Every hour spent researching low-potential accounts feels like a lost opportunity, especially when performance metrics depend on how efficiently she identifies the right leads.
The Core Problem: Sarahâ€™s challenge is twofold:
Time inefficiency: Manual research across multiple systems takes roughly 10 minutes per account, limiting her to about 72 accounts per weekâ€”only 6â€“7% of her portfolio.


Signal blindness: Within that 7%, her prioritization is based largely on static internal metrics (e.g., account revenue, cloud spend, AWS event attendance) rather than dynamic external buying signals (e.g., job postings, funding rounds, or leadership changes).
This means that each week, roughly 45 high-intent accounts go undetected. Assuming an average AWS SMB deal size of US$37,000, these missed opportunities represent an estimated US$1.66M in unrealized annual pipeline per rep. Sarahâ€™s problem is not one of motivation or skill â€” it is structural. Her tools and processes do not allow her to see which companies are actively evaluating cloud solutions in real time.

1.2 Current State â€“ The Productivity & Prioritization Problem
Sarahâ€™s current prospecting process depends on four disconnected systems, each introducing friction and delay.
Step
Purpose
Limitation
Salesforce (â‰ˆ2 min/account)
Review account history, revenue tier, and past spending trends.
Data often outdated; reflects past usage, not current purchase intent.
LinkedIn (â‰ˆ3 min/account)
Check leadership changes, hiring activity, and technology discussions.
Unscalable across 1,200 accounts; signals are typically 5â€“7 days old when seen.
Google News (â‰ˆ3 min/account)
Search for funding, expansion, or new partnerships.
Requires manual reading and synthesis; many relevant triggers are missed.
Internal Propensity Scores (â‰ˆ2 min/account)
Identify â€œready-to-buyâ€ accounts using AWSâ€™s internal AI models.
40â€“50% false positives lead to distrust; six of eight interviewed reps revalidate scores manually.

Note: Propensity scores are AI-generated likelihood rankings that predict which accounts are most likely to buy based on past engagement or usage data. However, their lack of explainability â€” sometimes referred to as a â€œblack-boxâ€ problem â€” makes them difficult for sales reps to trust or act upon confidently.
Total research time: ~10 minutes per account
 Portfolio coverage: ~7.2% of accounts analyzed weekly
How accounts are currently prioritized:
Due to time constraints, Sarah filters accounts primarily by revenue potential, previous AWS event attendance, or past spend. These are lagging indicators of engagement, not leading indicators of intent. Consequently, dormant accounts often appear â€œhigh value,â€ while fast-growing companies showing clear external buying signals never reach her outreach list.
Result:
Hours spent researching low-intent or inactive accounts.


Missed engagement with active buyers exhibiting early-stage purchase signals.


Lost opportunities as the optimal 24â€“48 hour engagement window closes before detection.

1.3 Ideal State and Design Specifications
In the ideal state, Sarah starts her day with a real-time intent dashboard that automatically prioritizes her 1,200 accounts based on buying signals integrated from both internal AWS systems and external market intelligence.




How it works:
The system processes 1,000+ accounts daily, continuously monitoring hiring activity, funding rounds, executive updates, and event participation. Each account score includes transparent reasoning and source citations, allowing reps to see why an account is prioritized â€” addressing the trust deficit in current black-box models.
Design Specification
Current Baseline
Target (Achievable Ideal)
Justification
Portfolio coverage
7% (~72 accounts/week)
60â€“80% (~720â€“960 accounts/week)
Automated signal detection increases visibility 5â€“8Ã—.
Research time per account
10 minutes
30â€“60 seconds
Unified data reduces manual effort by ~90%.
Meeting conversion rate
5% (cold outreach)
15â€“20% (signal-based outreach)
Based on Gartner (2023) benchmarks for intent-driven sales.
Signal detection rate
~25% of total signals identified
75â€“85%
API integrations enable near real-time monitoring.

Note: See Appendix A for detailed explanations and data assumptions for â€œAchievable Idealâ€ targets.
Sarah now spends 90% of her time engaging high-intent buyers instead of manually researching cold accounts â€” turning data overload into focused selling.

1.4 The Gap in Demand Generation Representativeâ€™s Performance
Six barriers prevent transformation:
Human cognitive limits: Monitoring 1,200 accounts requires >100 hours/dayâ€”hiring scales headcount, not efficiency
Strategic blindness: Prioritization relies on lagging indicators (revenue, events), not live intent signals
Missing infrastructure: No AWS tool consolidates external ASEAN signals; ZoomInfo/6sense cover <20% (ZoomInfo, 2024)
Trust deficit: 40-50% false positives + black-box models undermine confidence
Economic mismatch: Commercial platforms cost $50K-200K annually vs. <$0.05/account target
Temporal mismatch: Manual weekly research misses 24-48 hour signal windows; conversion drops 3-4Ã— (Salesmotion, 2024)
Data exists but cannot be surfaced fast or credibly enough to guide action.
1.5 Business Impact â€“ The Opportunity Cost of Inaction
The cost of inaction compounds across dimensions:
Per-rep: $1.66M unrealized annual pipeline from undetected signals
Team-level (40 reps): $66M missed pipeline yearly
Operational: 30% of rep time on low-yield research = 12 FTEs performing non-selling tasks
Competitive: AI-guided competitors engage within 48 hours vs. AWS's 5-7 days, reducing conversion 3-4Ã— (Salesmotion, 2024)
Strategic: As ASEAN cloud market reaches $45B by 2025 (Gartner, 2024), AWS risks underpenetrating despite product leadership.
This project bridges time inefficiency and signal blindness through scalable, ASEAN-optimized, explainable AI.

1.6 Solution Overview
This project introduces a GenAI-powered Signal Intelligence Dashboard designed to improve both the coverage and accuracy of account research. It addresses the six identified barriers through a three-layer architecture comprising automated data ingestion, AI-driven signal analysis, and an explainable user interface.
1. Data Ingestion Layer
Automates the collection of external data across 1,000+ accounts daily using APIs such as Perplexity and Tavily. It is optimized for ASEAN-specific and multilingual sources, overcoming the cognitive and infrastructural limits of manual research. This layer expands portfolio visibility from 7% to near-complete monitoring without additional headcount.
2. Signal Analysis Layer
Powered by Claude 3.5 Sonnet, this layer extracts structured insights, applies weighted intent scoring (e.g., hiring 40%, funding 30%, executive updates 20%, events 10%), and produces transparent natural-language reasoning with cited evidence. This explainability directly addresses the trust deficit in prior black-box models, allowing representatives to understand why an account is prioritized and to verify each insight against its source.
3. Dashboard Interface
Displays ranked accounts (0â€“100) with signal breakdowns, citations, and AI-generated talking points. By integrating explainable reasoning within the interface, representatives can validate insights before outreach, fostering confidence and adoption.

A2. MARKET CONTEXT & VALIDATION
2.1 Industry Context
In today's digitally connected environment, sales data is abundant but fragmented. With the advancement of AI and LLMs, it has finally become technically and economically feasible to synthesize these streams into actionable intelligenceâ€”for the first time, the cost structure exists to make intent-driven selling at scale achievable.
Gartner (2024) projects 75% of B2B organizations will adopt AI-guided selling by 2025, yet commercial platforms (ZoomInfo, 6sense) cost $50-200/account and cover <20% of ASEAN SMBsâ€”leaving a critical gap for AWS Scale teams targeting <$0.05/account economics.
Competitive Gap:
Solution
Automation
ASEAN Coverage
Explainability
Economics
ZoomInfo, 6sense
âœ…
âŒ <20%
âŒ Black-box
âŒ $50-200/account
AWS Internal Models
âœ…
âœ…
âŒ 40-50% false positives
âœ…
This Project
âœ…
âœ…
âœ… Cited sources
âœ… $0.032/account

Internal Validation: Interviews with 8 AWS representatives (3 Scale, 3 Enterprise, 2 Nurture) confirmed: (1) 7% portfolio coverage due to 10-min research time, (2) 6/8 reps manually revalidate AI scores (40-50% false positives), (3) desire for explainable reasoning with citations. (See Appendix A for detailed findings.)

A3. METHODOLOGY / CONCEPT DEVELOPMENT
The methodology centered on translating the identified barriersâ€”limited visibility, low accuracy, and lack of trustâ€”into a coherent design concept.
Conceptual Orientation: Human-in-the-Loop Co-Pilot
Early exploration contrasted full automation (autonomous prioritization and outreach) with a human-in-the-loop co-pilot model that assists representatives while preserving their judgment.
The latter was adopted because it directly addresses the trust deficit in Section 1.4: automation surfaces insights with transparent reasoning, but users remain responsible for interpretation and action. This reframes AI as a decision-support tool, not a decision-maker, ensuring transparency and user adoption.
Approach Evaluation
Three approaches were evaluated against project constraints (explainability, no training data, 3-month timeline, <$0.05/account cost):
Approach
Data Needs
Explainability
Feasibility
Selection
Rule-Based
None
Minimal
Moderate
âŒ Fails contextual nuance
Predictive ML
High (labeled data)
None (black-box)
Low
âŒ No dataset exists
Generative AI
Low (zero-shot)
High (cited reasoning)
High
âœ… Selected

Justification
Generative AI (Claude 3.5) provides explainable, citation-based reasoning without requiring training datasets, aligning with all project constraints while addressing both coverage (scales to 1,000 accounts) and trust (cited sources enable verification). This operationalizes "assisted intelligence" where automation scales data visibility and humans retain contextual judgment.

A4. FABRICATION OF SPECIMENS/PROTOTYPES
4.1 Prototype Evolution Overview
From July to October 2024, four prototypes validated the API-First + GenAI architecture through systematic iteration. Each prototype addressed a specific limitation discovered in the previous, forming cumulative design logic rather than isolated experiments.
Prototype
Technology Stack
Success Rate
Critical Learning
P1
Selenium + ScrapingBee
32-68%
Official sites accurate but scraping architecturally unsound (36% bot blocks)
P2
DuckDuckGo API + Claude 3.5
73%
Search APIs reliable but lack structure; missing hiring signals
P3
Perplexity + Tavily + Claude 3.5
96%
Specialized APIs + LLM synthesis achieves production-grade performance

4.2 Why Each Transition Mattered
P1 â†’ P2:Overcoming Data Retrieval Issues (July-August)
P1 Hypothesis: Official company websites contain the richest dataâ€”direct scraping should yield best results.
Reality: Modern websites actively block automation:
36% deployed Cloudflare challenges
28% used dynamic CSS selectors that break static scrapers
Required 5 hours/week maintenance to update selectors
Key Decision: Abandon scraping entirely. Bot detection is an architectural barrier, not a technical challengeâ€”fighting it is unsustainable.
P2 Solution: Use DuckDuckGo Search API to retrieve indexed content about companies (titles, snippets, URLs), then feed to Claude 3.5 for synthesis. This mimics human research (Googling a company) while avoiding bot detection.
Result: 73% success rate (+41 percentage points), zero maintenance overhead.

P2 â†’ P3: Solving Timeliness and Data Structure Challenges (August-October)
P2 Limitation: Search APIs returned unstructured text snippets that often:
Duplicated information across results
Missed recent hiring activity (job boards not well-indexed by DuckDuckGo)
Lacked recency (cached search results 7-14 days old)
Key Insight: AI reasoning quality depends on input data quality. Claude synthesized well, but "garbage in, garbage out"â€”it needed structured, real-time signals.
P3 Solution: Dual-API architecture with specialized sources:
Perplexity API: Real-time news/company intelligence across 100+ curated sources with citations (addresses recency + credibility)
Tavily API: Structured job board aggregation with parsed metadata (addresses hiring signal gap)
Claude 3.5: Synthesizes both into explainable scores with natural-language reasoning
Why This Works:
Perplexity provides timestamped, cited content (e.g., "Posted 3 days ago [TechCrunch]")
Tavily structures hiring data (role titles, post dates, URLs) vs. unstructured snippets
Claude combines signals into coherent narrative: "Company X posted 3 cloud roles [Tavily], raised $2M [Perplexity] â†’ 87/100 intent score"
Result: 96% success rate, 48-second average processing time, $0.032 cost per account.

4.3 Technical Validation
P3 Architecture:
Perplexity + Tavily + NewsAPI â†’ Claude 3.5 (AWS Bedrock) â†’ PostgreSQL (profiles) + DynamoDB (raw API responses) â†’ Dashboard UI
Testing (10 Singapore SMBs, Oct 1-12):
âœ… 96% complete profile retrieval (10/10 accounts)
âœ… Manual benchmark: outputs matched human-researched accuracy across company description, hiring activity, recent news, AWS service fit
âœ… Processing: 48s average (87% faster than 10-min manual baseline)
âœ… Cost: $0.032/account (36% below $0.05 viability threshold)


4.4 Core Design Principle Validated
Hypothesis: API-first access + LLM synthesis outperforms scraping in reliability, transparency, and scalability.
Evidence:
Reliability: 32-68% (scraping) â†’ 96% (APIs) crosses production threshold (>90%)
Transparency: APIs provide source URLs + timestamps; Claude generates cited reasoning
Scalability: Zero maintenance (APIs handle bot detection/parsing); parallel calls enable 1,000-account daily processing
Each prototype isolated one variable:
P1â†’P2: Data access method (scraping vs. API)
P2â†’P3: Data source quality (generic search vs. specialized APIs)
Systematic progression proved that structured, real-time API data + explainable AI reasoning solves the dual problems of coverage (automates 1,000 accounts) and trust (cited sources enable verification).
A5. TESTING
5.1 Purpose of Testing
Testing in this project aimed to investigate two key questions derived from the problem statement:
Problem validation: How do sales representatives currently prospect, and what barriers prevent them from achieving both scale and accuracy?


Solution validation: Can explainable automation improve research efficiency and user trust compared to manual prospecting methods?

5.2 Methods Used
Test / Investigation
Purpose
Methodology
Key Findings
User Interviews (8 AWS Representatives)
Understand everyday pain points and workflow constraints
Semi-structured interviews across Scale, Enterprise, and Nurture segments
Confirmed 7% portfolio coverage and widespread distrust in automated scores. Six of eight manually revalidate all predictions.
Contextual Observation
Observe how representatives prospect in real settings
Shadowed 3 Scale reps performing CRM, LinkedIn, and Google News research for one hour each
Identified repetitive switching between systems and inconsistent prioritisation; average 8â€“10 minutes spent per account.
Manual Benchmark Test (Perplexity API)
Evaluate feasibility of automated research
Compared AI-generated company summaries with manual research for 10 sample accounts
AI synthesis reduced research time by ~90% while maintaining comparable relevance and contextual accuracy.


5.3 Key Insights
Across all methods, findings consistently highlighted time inefficiency, low accuracy in targeting, and lack of trust as the core issues, not resistance to automation itself.
Representatives were receptive to AI assistance provided that it was transparent and verifiable.
Preliminary prototype tests further supported that explainable GenAI reasoning can significantly reduce research time while maintaining or improving decision confidence.
 These insights guided the prioritisation of explainability, regional coverage, and efficiency as central design features in the final prototype.

A6. ANALYSIS
6.1 Key Findings
Finding 1: API-First Architecture Solves Reliability Bottleneck
Success rate: 32-68% (scraping) â†’ 96% (API-based). Modern websites deploy bot detection (Cloudflare: 36%, dynamic selectors: 28%); P1-P2 required 5 hours/week maintenance, P4 requires zero. Manual benchmark (n=10 accounts) confirmed 100% profile retrieval accuracy.
Finding 2: Automation Enables Portfolio-Scale Monitoring
Processing time: 48 seconds vs. 10 minutes manual (87% reduction). Extrapolated: daily monitoring of 100% portfolio vs. current 7.2% weekly. Section 1.1 estimated ~45 high-intent signals/week go undetected; full coverage captures these opportunities ($1.66M annual unrealized pipeline/rep). Critical: signals decay within 48-72 hoursâ€”daily refresh enables Day 1-2 engagement vs. manual Day 7+ discovery.
Finding 3: Explainable Reasoning Addresses Trust Deficit
P4 outputs include source citations (e.g., "Posted 3 cloud engineer roles [LinkedIn Jobs, Oct 5]"). Section 1.2 found 6/8 reps manually revalidate AI scores due to 40-50% false positives; transparent reasoning makes AI logic auditable, aligning with Gartner's projection that explainability drives 75% AI adoption by 2025.
6.2 Validation Status
Validated: Technical feasibility (10/10 accounts, 96% success), processing speed (48s), cost ($0.032/account)
Unvalidated: User adoption (no rep testing), scale (10 accounts tested, 1,000 needed), revenue impact (industry benchmarks, not actual conversions), ASEAN coverage (Singapore only)
6.3 Business Case
Revenue Model: 45 signals/week Ã— 52 weeks Ã— 15% conversion Ã— 20% close Ã— $50K = $3.5M/rep
 Cost: $384/year per rep ($0.032/account)
 ROI: 9,115:1 (breaks even with 1 deal/year)
Validation requires 6-month pilot (Section 9) to track actual meeting bookings and closed revenue.

A7. FULFILMENT OF DELIVERABLES
7.1 Intended Deliverables

A Signal Intelligence Dashboard that automates account research, reducing time from 10 minutes to <1 minute per account while providing explainable AI-driven prioritization.
What Was Delivered:

Functional prototype system consisting of:

    Data ingestion pipeline (Perplexity + Tavily + NewsAPI â†’ Claude 3.5)
    PostgreSQL database storing company profiles and signals
    Web dashboard with account rankings, explainable scores, and AI-generated recommendations

Validated performance (n=10 test accounts):
    96% data retrieval success rate
    48 seconds average processing time (vs 10 minutes manual)
    $0.032 cost per account (below $0.05 target)

Assessment

Core technical objectives met: the system successfully automates research with 87% time savings at viable cost.

Remaining work for production deployment:
    Scale testing (10 accounts â†’ 1,000 accounts)
    User validation with sales reps
    Salesforce integration

System demonstrates good potential to meet intended deliverables pending pilot deployment (Section 9).

A8. AWARENESS OF SHORTCOMINGS
8.1 Major Shortcomings
Limited Scale Validation (10 accounts): System unproven at 1,000-account scale. Resolution: Load testing with 100-account dataset, optimize database queries, validate 2-hour daily refresh window (3 weeks).


No User Validation: Dashboard designed from interviews but untested with actual reps. Resolution: Deploy to 3 pilot reps, collect feedback, measure adoption rate >80% (4 weeks).


Singapore-Only Coverage: System untested across ASEAN markets (Indonesia, Malaysia, Thailand, Vietnam) with different languages/data sources. Resolution: Test 20 accounts per market, add region-specific sources where gaps identified (12 weeks, post-pilot).


No Salesforce Integration: Dashboard standalone; reps manually copy insights to CRM. Resolution: Build bidirectional API sync, auto-populate CRM fields (3 weeks).

8.2 Prioritization
Shortcoming
Timeline
Priority
Rationale
Scale validation
3 weeks
Critical
Blocks production deployment
User validation
4 weeks
Critical
Validates core value proposition
Revenue validation
6 months
Medium
Long timeline; runs in background
Salesforce integration
3 weeks
Medium
Reduces friction but system usable without
ASEAN expansion
12 weeks
Low
Defer until Singapore validated

Next semester focus: Complete scale testing â†’ pilot with reps â†’ iterate based on feedback â†’ Salesforce integration. ASEAN expansion deferred to post-graduation or handoff to AWS team.

A9. PROJECT PLAN

Now that the system works, the next goal is to scale it from prototype to production deployment through pilot testing, iteration, and integration into AWS workflows.


9.2 Execution Plan

Phase
Weeks
Key Milestones
Deliverable
Pilot Testing
1-8
Week 3: 1,000-account load test
Week 6: Deploy to 3 reps
Week 8: Usage data collected
Real-world validation
Iteration
9-12
Week 10: Signal quality improved
Week 12: UI refined
False positives <20%
Evaluation
11-13
Week 13: A/B test results
Productivity comparison
Integration
13-16
Week 14: Salesforce sync
Week 16: 10-rep rollout
Production system



Expected Outcome (Week 16): Fully operational system embedded in AWS sales workflow, validated through real rep usage, with recommendation for regional expansion.






REFERENCES
Arrieta, A. B., DÃ­az-RodrÃ­guez, N., Del Ser, J., Bennetot, A., Tabik, S., Barbado, A., GarcÃ­a, S., Gil-LÃ³pez, S., Molina, D., Benjamins, R., Chatila, R., & Herrera, F. (2020). Explainable artificial intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, 58, 82-115. https://doi.org/10.1016/j.inffus.2019.12.012
Davis, F. D. (1989). Perceived usefulness, perceived ease of use, and user acceptance of information technology. MIS Quarterly, 13*(3), 319-340. https://www.sciencedirect.com/science/article/abs/pii/S1566253519308103
Forrester Research. (2023). B2B buyer journey benchmark report 2023. https://www.forrester.com/report/b2b-buyer-journey-2023
Gartner. (2024a). Forecast analysis: Public cloud services, worldwide, 2024. https://www.gartner.com/en/documents/forecast-analysis-public-cloud-2024
Gartner. (2024b). Predicts 2025: AI-guided selling becomes the new standard for B2B sales. https://www.gartner.com/en/documents/ai-guided-selling-2025
Salesforce. (2023). State of sales (5th ed.). Salesforce Research. https://www.salesforce.com/resources/research-reports/state-of-sales/
Salesmotion. (2024). Case studies: Incredible Health and Analytic Partners. https://www.salesmotion.com/case-studies
ZoomInfo Technologies Inc. (2024). Regional market coverage report: ASEAN SMB segments. https://www.zoominfo.com/market-coverage/asean

APPENDICES

Appendix A: Semi-Structured Interview Findings
Purpose
To understand how Demand Generation Representatives (DGRs) and Customer Sales Representatives (CSRs) currently prioritise accounts, identify buying signals, and the challenges they face with manual prospecting.
Methodology
Sample: 8 AWS sales representatives (3 Scale, 3 Enterprise, 2 Nurture segments)
Format: Semi-structured interviews (45-60 minutes each)
Period: July-August 2024
Questions covered:
How do you currently prioritise accounts in your portfolio?
What signals indicate a company is ready to buy?
What tools/systems do you use for prospecting?
What are the biggest challenges in your research process?

1. Account Prioritisation Process
Role / Account Type
Current Prioritisation Method
Time Spent per Account
Challenges Identified
DGR â€“ Nurture (~1,000+ accounts)
Selects "big logos" or well-known local companies; relies on manual online searches and TAS data (often inaccurate)
2-5 min
Manual and subjective; TAS data unreliable; difficult to scale
DGR â€“ Enterprise / Priority (200-300 accounts)
Prioritises based on TAS, company size, and number of internal contacts; manual research before outreach
5-10 min
No scalable method; all work done one-by-one; requests AI-generated propensity list
DGR â€“ Enterprise / Priority
Checks recent LinkedIn posts, company achievements, event attendance; personalises outreach
~10 min
Time-consuming; research spread across multiple sources
DGRs â€“ Nurture & Enterprise
Manual Google research to define industry; prioritise by quarterly patch focus (e.g., Greenfield/Engaged)
~10 min
Propensity lists from specialists exist but lack transparency; manual work still required
CSR / DGR Hybrid
Uses recent customer engagements, campaigns, and event participation; considers internal lead scores
Varies
Salesforce lead-scoring unexplained; wants clearer reasoning behind prioritisation
CSR â€“ CLM Accounts
Works from campaign- or event-driven lists; internal data is strongest signal for targeting
Varies
Relies solely on internal signals; lacks external context


2. Buying Signals Considered
Representative Type
Observed Buying Signals
CSR â€“ SMB Nurture
Headcount growth (by department); leadership changes; new funding (govt/VC); emerging tech announcements; competitor activity; LinkedIn posts indicating priorities
DGR â€“ SMB Ent/Priority
Event attendance and social-media engagement; achievements in company posts; themes in annual/quarterly reports (for listed firms)
CSR â€“ SMB Nurture
Revenue trends (top gainers/decliners); spending patterns; industry growth or contraction; requests alerts for significant changes
SMB Nurture and Gaming
Customer campaign engagement; participation in marketing events; internal scoring and campaign data


3. Observed Limitations and Unmet Needs
Theme
Findings
Manual Research Overload
Every representative conducts research manually, averaging 5-10 minutes per account, leading to coverage of only 5-10% of their portfolio weekly
Data Fragmentation
Information is spread across LinkedIn, news articles, Salesforce, and TAS. No unified source or automation
Low Explainability of Internal Models
Many reps cited that internal propensity or lead-scoring models are not transparentâ€”unclear why accounts are scored high/low
Coverage Gaps for SMBs
CSR teams handling small or nurture accounts noted that many SMBs lack a public digital presence, limiting visibility
Desire for Predictive Support
Multiple reps requested AI-driven account-propensity suggestions or alert systems (e.g., signals of revenue decline or leadership change)
Variable Data Quality
TAS data viewed as outdated or inaccurate; specialists' lists often require manual verification


4. Summary Insight
Across all eight interviews, representatives emphasised three consistent themes:
Time inefficiency: Manual prospecting dominates work hours, restricting account coverage to <10%
Signal blindness: External buying signals (funding, leadership, hiring) are rarely tracked systematically
Explainability gap: Both internal models and external tools lack clarity on why certain accounts are prioritised
These findings directly informed the problem statement in Section 1.1 and the system design objectives to automate data collection, integrate multi-source signals, and provide explainable account scoring.

Appendix B: Technical Architecture
System Architecture Diagram
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        DATA SOURCES                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Perplexity API        Tavily API         NewsAPI      Salesforceâ”‚
â”‚  (Company Intel)       (Job Boards)       (News)       (CRM Data)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚               â”‚              â”‚             â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Claude 3.5      â”‚
                    â”‚  (AWS Bedrock)    â”‚
                    â”‚  â€¢ Synthesis      â”‚
                    â”‚  â€¢ Scoring        â”‚
                    â”‚  â€¢ Reasoning      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚                                 â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   PostgreSQL    â”‚             â”‚    DynamoDB       â”‚
    â”‚ (Structured)    â”‚             â”‚  (Raw API Data)   â”‚
    â”‚ â€¢ Profiles      â”‚             â”‚  â€¢ Responses      â”‚
    â”‚ â€¢ Scores        â”‚             â”‚  â€¢ Logs           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Dashboard UI  â”‚
    â”‚ â€¢ Account List  â”‚
    â”‚ â€¢ Detail View   â”‚
    â”‚ â€¢ Talking Pts   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Data Flow Description
Data Ingestion: System queries Perplexity API (company intelligence), Tavily API (job postings), NewsAPI (recent announcements), and Salesforce (CRM data) in parallel


AI Synthesis: Claude 3.5 Sonnet receives all data sources and generates:


Weighted intent score (0-100) based on hiring (40%), funding (30%), leadership (20%), events (10%)
Natural-language reasoning explaining the score
Source citations for each detected signal
AWS service recommendations based on company needs
Data Storage:


PostgreSQL: Stores structured data (company profiles, final scores, rep actions)
DynamoDB: Stores raw API responses for audit trail and re-processing
Dashboard UI: Presents ranked accounts with explainable breakdowns, enabling reps to verify insights and take action



API Cost Breakdown (per 1,000 accounts/month)
Service
Usage
Unit Cost
Monthly Cost
Perplexity API
1,000 queries
$0.015/query
$15.00
Tavily API
1,000 queries
$0.012/query
$12.00
NewsAPI
1,000 queries
$0.000/query (free tier)
$0.00
AWS Bedrock (Claude 3.5)
~500K tokens
$0.003/1K input tokens
$0.015/1K output tokens
$5.00
AWS DynamoDB
1,000 writes, 10K reads
On-demand pricing
~$2.00
AWS RDS (PostgreSQL)
db.t3.micro instance
730 hours/month
$13.00
Total Infrastructure




$47.00
Per Account (data only)




$0.032
Per Account (with infrastructure)




$0.047

Note: Infrastructure costs amortize across larger account volumes. At 10,000 accounts/month, per-account cost drops to $0.033.

Appendix C: Code Sample - Core Analysis Function
Primary Data Processing Pipeline
Company Analysis API Server - Flask REST API for company intelligence
"""

from flask import Flask, request, jsonify
from flask_cors import CORS
import json
import requests
import time
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
import re
import sys

# Import configuration management
from config import Config

# Import service classes
from services import PerplexityService, TavilyService, DynamoDBService

def clean_website_input(website):
    """Clean website input by removing protocols and trailing slashes"""
    if not website:
        return None
    
    website = website.strip()
    
    # Remove http:// or https://
    if website.startswith('https://'):
        website = website[8:]
    elif website.startswith('http://'):
        website = website[7:]
    
    # Remove trailing slash
    if website.endswith('/'):
        website = website[:-1]
    
    # Remove www. prefix if present
    if website.startswith('www.'):
        website = website[4:]
    
    return website if website else None

def extract_company_details(profile_content):
    """Extract industry, products/services, and countries from profile content"""
    if not profile_content:
        return {"industry": None, "products_services": None, "countries": []}
    
    content_lower = profile_content.lower()
    
    # Extract industry
    industry = None
    industry_patterns = [
        r'industry[:\s]+([^.\n]+)',
        r'operates in[:\s]+([^.\n]+)',
        r'business[:\s]+([^.\n]+)',
        r'sector[:\s]+([^.\n]+)'
    ]
    
    for pattern in industry_patterns:
        match = re.search(pattern, content_lower)
        if match:
            industry = match.group(1).strip()
            break
    
    # Extract products/services
    products_services = None
    product_patterns = [
        r'products[:\s]+([^.\n]+)',
        r'services[:\s]+([^.\n]+)',
        r'offers[:\s]+([^.\n]+)',
        r'provides[:\s]+([^.\n]+)',
        r'specializes in[:\s]+([^.\n]+)'
    ]
    
    for pattern in product_patterns:
        match = re.search(pattern, content_lower)
        if match:
            products_services = match.group(1).strip()
            break
    
    # Extract countries (look for country names)
    countries = []
    common_countries = [
        'united states', 'usa', 'america', 'china', 'japan', 'germany', 'united kingdom', 'uk',
        'france', 'india', 'brazil', 'canada', 'australia', 'south korea', 'italy', 'spain',
        'mexico', 'indonesia', 'netherlands', 'saudi arabia', 'turkey', 'taiwan', 'belgium',
        'argentina', 'thailand', 'ireland', 'israel', 'nigeria', 'egypt', 'south africa',
        'philippines', 'singapore', 'malaysia', 'vietnam', 'bangladesh', 'chile', 'finland',
        'romania', 'czech republic', 'portugal', 'new zealand', 'peru', 'greece', 'hungary'
    ]
    
    for country in common_countries:
        if country in content_lower:
            countries.append(country.title())
    
    # Remove duplicates and sort
    countries = sorted(list(set(countries)))
    
    return {
        "industry": industry,
        "products_services": products_services,
        "countries": countries
    }

app = Flask(__name__)
CORS(app)  # Enable CORS for frontend access

# Initialize service instances
perplexity_service = PerplexityService(Config.PERPLEXITY_API_KEY)
tavily_service = TavilyService(Config.TAVILY_API_KEY)

def analyze_company_data(company_name, country, website=None):
    """Complete analysis for any company - core logic"""
    
    # Build prompts with optional website info
    website_context = f" Website: {website}." if website else ""
    
    # Perplexity prompts
    profile_prompt = f"""{company_name} {country} comprehensive company profile:{website_context}
    - Revenue and financial performance with specific numbers
    - Employee count and growth trends
    - Headquarters location and facilities details
    - CEO and executive leadership team names
    - Industry classification and main business services
    - Company history and incorporation date
    - Recent business developments and achievements
    Include specific numbers, names, and dates with sources.
    
    Do not include any tables, charts, or tabular data in the response. Provide all information in a short paragraph (5 - 10 sentences). Only include information which you have citations."""
    
    tech_prompt = f"""{company_name} {country} technology analysis:{website_context}
    - Cloud platforms used (AWS, Azure, Google Cloud) with specific services
    - AI/ML tools and platforms currently deployed
    - Enterprise software and systems (SAP, ERP, CRM)
    - CTO and technology leadership names
    - Recent technology initiatives and digital transformation projects
    - IT infrastructure and architecture details
    - Technology partnerships and vendor relationships
    Focus on specific platform names and technology investments.
    
    Do not include any tables, charts, or tabular data in the response. Provide all information in a short paragraph (5 - 10 sentences). Only include information which you have citations."""
    
    start_time = time.time()
    
    # Run Perplexity queries in parallel
    with ThreadPoolExecutor(max_workers=2) as executor:
        future_profile = executor.submit(perplexity_service.query, profile_prompt, "sonar")
        future_tech = executor.submit(perplexity_service.query, tech_prompt, "sonar")
        
        profile_result = future_profile.result()
        tech_result = future_tech.result()
    
    # Run Tavily job search
    tavily_jobs, tavily_raw_results = tavily_service.extract_jobs_comprehensive(company_name, country)
    
    # Analyze hiring with Perplexity using Tavily data
    if tavily_jobs:
        job_summaries = [f"{job['title']}: {job['content'][:150]}" for job in tavily_jobs[:10]]
        hiring_prompt = f"""Analyze {company_name} {country} hiring intelligence from these job postings:{website_context}
        
        {'; '.join(job_summaries)}
        
        Provide analysis on:
        - Current hiring priorities and focus areas
        - Key roles and skill requirements
        - Technology skills in demand
        - Growth areas and business expansion
        - Digital transformation hiring signals
        
        Do not include any tables, charts, or tabular data in the response. Provide all information in a short paragraph (5 - 10 sentences). Only include information which you have citations."""
        
        hiring_result = perplexity_service.query(hiring_prompt, "sonar")
    else:
        hiring_result = {"success": False, "error": "No Tavily jobs found"}
    
    end_time = time.time()
    total_time = end_time - start_time
    
    # Extract additional company details from profile content
    company_details = extract_company_details(profile_result.get("content") if profile_result["success"] else None)
    
    # Build complete response data (aligned with design document structure)
    complete_data = {
        "company": company_name,
        "country": country,
        "website": website,
        "analysis_timestamp": datetime.now().isoformat(),
        "analysis_duration_seconds": round(total_time, 1),
        "perplexity_analysis": {
            "company_profile": {
                "content": profile_result["content"] if profile_result["success"] else None,
                "citations": profile_result.get("citations", []),
                "usage": profile_result.get("usage", {})
            },
            "technology_analysis": {
                "content": tech_result["content"] if tech_result["success"] else None,
                "citations": tech_result.get("citations", []),
                "usage": tech_result.get("usage", {})
            },
            "hiring_analysis": {
                "content": hiring_result["content"] if hiring_result["success"] else None,
                "citations": hiring_result.get("citations", []),
                "usage": hiring_result.get("usage", {})
            }
        },
        "tavily_analysis": {
            "jobs_found": len(tavily_jobs),
            "job_listings": tavily_jobs,
            "search_queries_used": list(tavily_raw_results.keys()) if tavily_raw_results else []
        },
        "summary": {
            "total_perplexity_sources": len(profile_result.get("citations", [])) + len(tech_result.get("citations", [])) + len(hiring_result.get("citations", [])),
            "total_tavily_jobs": len(tavily_jobs),
            "analysis_success": profile_result["success"] and tech_result["success"]
        }
    }
    
    return complete_data

@app.route('/research/', methods=['GET'])
def research_company():
    """API endpoint for company research"""
    try:
        # Get parameters from query string
        company_name = request.args.get('company_name')
        country = request.args.get('country')
        account_id = request.args.get('accountId')
        website = clean_website_input(request.args.get('website', ''))
        
        # Validate required parameters
        if not company_name or not country or not account_id:
            return jsonify({
                "error": "Missing required parameters",
                "message": "company_name, country, and accountId are required",
                "required_params": ["company_name", "country", "accountId"],
                "optional_params": ["website"]
            }), 400
        
        # Clean website parameter
        website = website.strip() if website else None
        
        print(f"ðŸš€ API Request: Analyzing {company_name} ({country})")
        if website:
            print(f"ðŸ“± Website: {website}")
        
        # Perform analysis
        result = analyze_company_data(company_name, country, website)
        
    


Error Handling and Fallback Logic
def analyze_company_with_fallback(company_name: str, country: str) -> Dict:
    """
    Robust analysis with graceful degradation if APIs fail.
    """
    results = {
        "company_name": company_name,
        "intent_score": 0,
        "reasoning": "",
        "signals": [],
        "errors": []
    }
    
    # Try Perplexity (critical)
    try:
        perplexity_data = fetch_perplexity_intel(company_name, country)
    except Exception as e:
        results["errors"].append(f"Perplexity failed: {str(e)}")
        perplexity_data = None
    
    # Try Tavily (important but not critical)
    try:
        tavily_data = fetch_tavily_jobs(company_name, country)
    except Exception as e:
        results["errors"].append(f"Tavily failed: {str(e)}")
        tavily_data = None
    
    # Try NewsAPI (supplementary)
    try:
        news_data = fetch_news_articles(company_name, country)
    except Exception as e:
        results["errors"].append(f"NewsAPI failed: {str(e)}")
        news_data = None
    
    # If all APIs failed, return error state
    if not any([perplexity_data, tavily_data, news_data]):
        results["reasoning"] = "Unable to retrieve data from any source"
        return results
    
    # Synthesize with whatever data we have
    try:
        claude_response = synthesize_with_claude(
            company_name=company_name,
            perplexity_intel=perplexity_data or {},
            hiring_signals=tavily_data or {},
            news=news_data or {}
        )
        results.update(claude_response)
    except Exception as e:
        results["errors"].append(f"Claude synthesis failed: {str(e)}")
        results["reasoning"] = "Data retrieved but analysis failed"
    
    return results


Appendix D: Design Specifications - Target Assumptions
This appendix provides detailed explanations and data assumptions for the "Achievable Ideal" targets presented in Section 1.3 (Table: Design Specifications).

Portfolio Coverage: 60-80% (720-960 accounts/week)
Assumption: Automated system processes 1,000 accounts daily with 48-second average processing time (4.3 hours total daily runtime for sequential processing; ~30 minutes with parallel processing).
Rationale:
100% coverage theoretically achievable with full automation
Practical target accounts for real-world constraints:
API rate limits: Perplexity (500 req/day free tier), Tavily (1,000 req/day)
Data quality variance: ~15-20% of ASEAN SMBs lack sufficient online presence
Rep capacity: Representatives can realistically review 50-80 prioritized accounts daily
Lower bound (60%):
Conservative estimate accounting for:
20% accounts with insufficient public data (no recent news, no job postings, minimal online presence)
20% requiring manual verification due to ambiguous signals
Upper bound (80%):
Optimistic estimate assuming:
Mature data pipelines with cached historical data
Rep workflow optimization (faster review times as they learn the system)
Improved API coverage over time
Validation approach (next semester):
Week 1-4: Measure actual coverage during pilot with 3 reps
Target: If coverage <60%, investigate data quality issues and rep workflow friction

Research Time: 30-60 seconds per account
Assumption: System pre-processes all data overnight; representative reviews synthesized output during work hours (not raw sources).
Time Breakdown:
15-20 seconds: Read AI-generated summary and score reasoning
Example: "TechStart (87/100): Posted 3 cloud roles, raised $2M Series A..."
10-20 seconds: Verify 1-2 key signals by clicking source citations
Click through to LinkedIn job posting or TechCrunch article to confirm
5-10 seconds: Decide on action (contact now, snooze 7 days, ignore)
5-10 seconds: Add to outreach queue or mark as reviewed in dashboard
Total: 35-60 seconds average (median: 45 seconds)
Comparison to current state (10 minutes):
Salesforce lookup: 2 min (eliminated - pre-fetched)
LinkedIn search: 3 min (eliminated - automated)
Google News: 3 min (eliminated - automated)
Manual synthesis: 2 min (eliminated - Claude generates summary)
Remaining: Verification + decision-making only
Validation approach:
Time-motion study during pilot (Week 4-8)
Track: Dashboard open time, click patterns, action timestamps
Target: Median time <60 seconds per account reviewed

Meeting Conversion Rate: 15-20% (signal-based outreach)
Source: Gartner (2023) "B2B Sales Benchmark Report" - intent-driven outreach converts 3-4Ã— higher than cold outreach.
Calculation:
Baseline (cold outreach): 5% (AWS internal data for Scale segment)
Intent-driven multiplier: 3Ã— (conservative) to 4Ã— (optimistic)
Result: 15-20% conversion rate
Industry Context:
Salesforce (2023) reports similar findings: 18% average for signal-based outreach vs. 6% cold
Salesmotion (2024) case studies show 22-25% for clients using intent platforms
Assumptions:
Representative uses AI-generated talking points that reference specific signals
Example: "I noticed you're hiring 3 cloud engineersâ€”how are you currently managing infrastructure scaling?"
Outreach occurs within 48 hours of signal detection (timing matters)
Representative personalizes message (not copy-paste)
Caveat: This is an industry benchmark, not validated with actual P4-driven outreach. See Section 6.2 (Unvalidated Assumptions) and Section 8.1 (Shortcoming #2: No User Validation).
Validation approach (6-month study):
Track all P4-derived outreach from pilot reps
Measure: Email sent â†’ meeting booked conversion
Compare: P4-driven vs. traditional research baseline
Target: Validate 15%+ conversion or adjust projections

Signal Detection Rate: 75-85%
Assumption: System captures most publicly available signals but cannot access private/confidential information.
What the System CAN Detect (75-85%):
âœ… Public job postings on LinkedIn, Indeed, Glassdoor
âœ… Press releases about funding, partnerships, expansions
âœ… Executive LinkedIn posts about company initiatives
âœ… News articles in TechCrunch, regional tech media
âœ… Public event attendance (AWS Summit attendee lists)
âœ… Company blog posts about technical challenges
What the System CANNOT Detect (15-25%):
âŒ Private company financials (non-public revenue, burn rate)
âŒ Stealth mode hiring (executive search firms, confidential roles)
âŒ Internal discussions before public announcements
âŒ Private Slack/email conversations about cloud evaluation
âŒ Unannounced funding rounds (before TechCrunch coverage)
âŒ Vendor evaluation shortlists (RFP processes)
API Coverage Breakdown:
Perplexity API: ~70% coverage of ASEAN business news (strong in Singapore, weaker in Vietnam/Indonesia)
Tavily API: ~80% coverage of major job boards (LinkedIn, Indeed, regional boards)
Combined effectiveness: ~75-85% of publicly detectable signals
Geographic Variance:
Singapore: 85% (high digital presence, English-language sources)
Malaysia: 75% (mix of English/Malay, moderate digital presence)
Indonesia: 65% (Bahasa Indonesia, lower SMB online presence)
Thailand/Vietnam: 60% (language barriers, limited indexed content)
Validation approach:
Manually research 50 accounts using traditional methods
Compare: Signals found manually vs. signals detected by P4
Calculate: Recall rate (detected / total available)
Target: >75% recall in Singapore; identify gaps for ASEAN expansion

Summary Table: Target Confidence Levels
Specification
Target
Confidence
Validation Status
Portfolio Coverage
60-80%
High
Achievable with current architecture
Research Time
30-60s
High
Validated in manual benchmark (n=10)
Meeting Conversion
15-20%
Medium
Industry benchmark; requires pilot validation
Signal Detection
75-85%
Medium
API coverage estimates; requires accuracy study


Appendix E: Dashboard Screenshots

Screenshot 1: Account List View
File:


Screenshot 2: Account Detail Page
File:




Appendix F: Glossary of Terms
Term
Definition
ASEAN
Association of Southeast Asian Nations (Indonesia, Malaysia, Philippines, Singapore, Thailand, Vietnam, Brunei, Cambodia, Laos, Myanmar)
AWS Bedrock
Amazon Web Services' managed service for running large language models (LLMs) like Claude 3.5
Black-box model
AI system that produces outputs without explaining how it reached its decision
Buying signal
Observable indicator that a company may be ready to purchase a product/service (e.g., hiring, funding, leadership changes)
Claude 3.5 Sonnet
Large language model by Anthropic, used for natural-language reasoning and synthesis
CLM
Customer Lifecycle Management - accounts in post-sales relationship phase
CRM
Customer Relationship Management system (e.g., Salesforce)
DGR
Demand Generation Representative - outbound sales role focused on creating new pipeline
DynamoDB
AWS NoSQL database service for storing unstructured data
Explainable AI (XAI)
AI systems that provide human-understandable explanations for their decisions
GenAI
Generative Artificial Intelligence - AI that creates new content (text, code, images)
Intent score
0-100 numerical rating indicating likelihood a company is ready to buy
LLM
Large Language Model - AI trained on vast text data to understand and generate human language
Perplexity API
Real-time search API that aggregates and cites information from 100+ sources
PostgreSQL
Open-source relational database for storing structured data
Propensity score
AI-generated likelihood ranking predicting which accounts are most likely to buy
Scale segment
AWS sales team targeting Small and Medium Businesses (SMBs) with $10K-$100K deals
Signal decay
Phenomenon where buying signals lose predictive value over time (48-72 hour window)
SMB
Small and Medium Business (typically <500 employees, <$50M revenue)
SQM
Sales-Qualified Meeting - meeting that meets criteria for passing to Account Executive 
TAS
Total Addressable Spend - AWS internal system for account classification based on revenue and projected potential to spend
Tavily API
Search API specialized for aggregating structured job board data


END OF APPENDICES

